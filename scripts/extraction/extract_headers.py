#!/usr/bin/env python3
"""
è¡¨å¤´æå–å’ŒæŒä¹…åŒ–å·¥å…·

è¯»å–æ•´ç†åçš„merged_dataset_simple.xlsxæ–‡ä»¶çš„è¡¨å¤´ç»“æ„ï¼Œ
å¹¶å°†å…¶ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­æ•°æ®åˆ†æä½¿ç”¨ã€‚

ä½œè€…: AICareç ”ç©¶å›¢é˜Ÿ
æ—¥æœŸ: 2025å¹´9æœˆ
"""

import pandas as pd
import json
from pathlib import Path
import sys
from datetime import datetime

def extract_headers_info(excel_file):
    """
    æå–Excelæ–‡ä»¶çš„è¡¨å¤´ä¿¡æ¯
    
    å‚æ•°:
        excel_file (Path): Excelæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        dict: åŒ…å«è¡¨å¤´ä¿¡æ¯çš„å­—å…¸
    """
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {excel_file}")
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(excel_file)
        
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"æ€»åˆ—æ•°: {len(df.columns)}")
        
        # æ„å»ºè¡¨å¤´ä¿¡æ¯å­—å…¸
        headers_info = {
            "metadata": {
                "file_name": excel_file.name,
                "extraction_date": datetime.now().isoformat(),
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "description": "AICare CHIè®ºæ–‡é—®å·æ•°æ®è¡¨å¤´ä¿¡æ¯"
            },
            "columns": []
        }
        
        # éå†æ¯ä¸€åˆ—ï¼Œæ”¶é›†è¯¦ç»†ä¿¡æ¯
        for i, col_name in enumerate(df.columns):
            col_info = {
                "index": i,
                "name": col_name,
                "data_type": str(df[col_name].dtype),
                "non_null_count": int(df[col_name].count()),
                "null_count": int(df[col_name].isnull().sum()),
                "unique_values_count": int(df[col_name].nunique())
            }
            
            # å¦‚æœæ˜¯æ•°å€¼å‹æ•°æ®ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if df[col_name].dtype in ['int64', 'float64']:
                col_info["statistics"] = {
                    "min": float(df[col_name].min()) if not df[col_name].empty else None,
                    "max": float(df[col_name].max()) if not df[col_name].empty else None,
                    "mean": float(df[col_name].mean()) if not df[col_name].empty else None
                }
            
            # å¦‚æœæ˜¯åˆ†ç±»æ•°æ®ä¸”å”¯ä¸€å€¼ä¸å¤šï¼Œæ·»åŠ å”¯ä¸€å€¼åˆ—è¡¨
            if df[col_name].nunique() <= 20 and df[col_name].nunique() > 0:
                unique_vals = df[col_name].dropna().unique().tolist()
                # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                col_info["unique_values"] = [str(val) for val in unique_vals]
            
            headers_info["columns"].append(col_info)
        
        return headers_info
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def categorize_columns(headers_info):
    """
    æ ¹æ®åˆ—åå¯¹åˆ—è¿›è¡Œåˆ†ç±»
    
    å‚æ•°:
        headers_info (dict): è¡¨å¤´ä¿¡æ¯å­—å…¸
    
    è¿”å›:
        dict: åˆ†ç±»åçš„åˆ—ä¿¡æ¯
    """
    categories = {
        "participant_info": [],      # å‚ä¸è€…åŸºæœ¬ä¿¡æ¯
        "risk_assessment": [],       # é£é™©è¯„ä¼°ç›¸å…³
        "nasa_tlx": [],             # NASA-TLXå·¥ä½œè´Ÿè·
        "sus_scale": [],            # ç³»ç»Ÿæ˜“ç”¨æ€§é‡è¡¨
        "trust_scale": [],          # è‡ªåŠ¨åŒ–ä¿¡ä»»é‡è¡¨
        "system_feedback": [],      # ç³»ç»ŸåŠŸèƒ½åé¦ˆ
        "collection_metadata": [],  # æ”¶é›†æƒ…å†µå…ƒæ•°æ®
        "other": []                 # å…¶ä»–
    }
    
    for col in headers_info["columns"]:
        col_name = col["name"].lower()
        
        # å‚ä¸è€…åŸºæœ¬ä¿¡æ¯
        if any(keyword in col_name for keyword in ["id", "æ€§åˆ«", "å¹´é¾„", "èŒç§°", "å·¥ä½œå¹´é™", "ä¸´åºŠä¸“ä¸š", "ç†Ÿæ‚‰ç¨‹åº¦"]):
            categories["participant_info"].append(col)
        # NASA-TLXç›¸å…³
        elif any(keyword in col_name for keyword in ["è„‘åŠ›éœ€æ±‚", "ä½“åŠ›éœ€æ±‚", "æ—¶é—´å‹åŠ›", "ä»»åŠ¡è¡¨ç°", "åŠªåŠ›ç¨‹åº¦", "æŒ«è´¥æ„Ÿ", "mental demand", "physical demand"]):
            categories["nasa_tlx"].append(col)
        # ç³»ç»Ÿæ˜“ç”¨æ€§é‡è¡¨
        elif "ç³»ç»Ÿæ˜“ç”¨æ€§é‡è¡¨" in col_name or "sus" in col_name:
            categories["sus_scale"].append(col)
        # è‡ªåŠ¨åŒ–ä¿¡ä»»é‡è¡¨
        elif "è‡ªåŠ¨åŒ–ä¿¡ä»»é‡è¡¨" in col_name or "trust" in col_name:
            categories["trust_scale"].append(col)
        # ç³»ç»ŸåŠŸèƒ½åé¦ˆ
        elif any(keyword in col_name for keyword in ["åŠŸèƒ½åé¦ˆ", "åŠ¨æ€é£é™©", "å¯è§†åŒ–", "å¤§è¯­è¨€æ¨¡å‹"]):
            categories["system_feedback"].append(col)
        # é£é™©è¯„ä¼°ç›¸å…³
        elif any(keyword in col_name for keyword in ["è¯„ä¼°", "é£é™©", "ä¿¡å¿ƒ", "ä¸´åºŠæŒ‡æ ‡", "æ‚£è€…", "å­•å¦‡"]):
            categories["risk_assessment"].append(col)
        # æ”¶é›†æƒ…å†µå…ƒæ•°æ®
        elif any(keyword in col_name for keyword in ["æ—¶é—´", "è®¿è°ˆ", "è®°å½•", "å¤‡æ³¨", "æ”¶é›†æƒ…å†µ"]):
            categories["collection_metadata"].append(col)
        else:
            categories["other"].append(col)
    
    return categories

def main():
    """ä¸»å‡½æ•°"""
    print("=== AICare è¡¨å¤´æå–å’ŒæŒä¹…åŒ–å·¥å…· ===")
    
    # æ–‡ä»¶è·¯å¾„
    base_dir = Path(__file__).parent.parent
    results_dir = base_dir / 'results'
    
    excel_file = results_dir / 'merged_dataset_simple.xlsx'
    json_file = results_dir / 'dataset_headers.json'
    categorized_json_file = results_dir / 'dataset_headers_categorized.json'
    
    print(f"è¾“å…¥æ–‡ä»¶: {excel_file}")
    print(f"è¾“å‡ºæ–‡ä»¶1: {json_file}")
    print(f"è¾“å‡ºæ–‡ä»¶2: {categorized_json_file}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not excel_file.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {excel_file}")
        print("è¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡åˆå¹¶è„šæœ¬å¹¶ç”Ÿæˆäº†merged_dataset_simple.xlsxæ–‡ä»¶")
        sys.exit(1)
    
    # æå–è¡¨å¤´ä¿¡æ¯
    print(f"\nğŸ“‹ å¼€å§‹æå–è¡¨å¤´ä¿¡æ¯...")
    headers_info = extract_headers_info(excel_file)
    
    if headers_info is None:
        print("âŒ æå–è¡¨å¤´ä¿¡æ¯å¤±è´¥")
        sys.exit(1)
    
    # ä¿å­˜åŸºç¡€è¡¨å¤´ä¿¡æ¯
    print(f"\nğŸ’¾ ä¿å­˜åŸºç¡€è¡¨å¤´ä¿¡æ¯åˆ°: {json_file}")
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(headers_info, f, ensure_ascii=False, indent=2)
        print("âœ… åŸºç¡€è¡¨å¤´ä¿¡æ¯ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¿å­˜åŸºç¡€è¡¨å¤´ä¿¡æ¯å¤±è´¥: {str(e)}")
        sys.exit(1)
    
    # å¯¹åˆ—è¿›è¡Œåˆ†ç±»
    print(f"\nğŸ·ï¸  å¯¹åˆ—è¿›è¡Œåˆ†ç±»...")
    categorized_info = {
        "metadata": headers_info["metadata"],
        "categories": categorize_columns(headers_info),
        "summary": {}
    }
    
    # æ·»åŠ åˆ†ç±»æ±‡æ€»ä¿¡æ¯
    for category, columns in categorized_info["categories"].items():
        categorized_info["summary"][category] = {
            "count": len(columns),
            "column_names": [col["name"] for col in columns]
        }
    
    # ä¿å­˜åˆ†ç±»åçš„è¡¨å¤´ä¿¡æ¯
    print(f"\nğŸ’¾ ä¿å­˜åˆ†ç±»è¡¨å¤´ä¿¡æ¯åˆ°: {categorized_json_file}")
    try:
        with open(categorized_json_file, 'w', encoding='utf-8') as f:
            json.dump(categorized_info, f, ensure_ascii=False, indent=2)
        print("âœ… åˆ†ç±»è¡¨å¤´ä¿¡æ¯ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ†ç±»è¡¨å¤´ä¿¡æ¯å¤±è´¥: {str(e)}")
        sys.exit(1)
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print(f"\nğŸ“Š è¡¨å¤´æå–å®Œæˆ!")
    print(f"   - æ€»åˆ—æ•°: {headers_info['metadata']['total_columns']}")
    print(f"   - æ€»è¡Œæ•°: {headers_info['metadata']['total_rows']}")
    print(f"\nğŸ·ï¸  åˆ—åˆ†ç±»æ±‡æ€»:")
    for category, info in categorized_info["summary"].items():
        if info["count"] > 0:
            print(f"   - {category}: {info['count']} åˆ—")
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - åŸºç¡€è¡¨å¤´: {json_file}")
    print(f"   - åˆ†ç±»è¡¨å¤´: {categorized_json_file}")

if __name__ == "__main__":
    main()
